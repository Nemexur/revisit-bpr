---
num_users: &num_users {{ (num_users | int) + 1 }}
num_items: &num_items {{ (num_items | int) + 1 }}

experiment:
  _target_: experiments.bpr.Experiment
  skip_seen: false
  metrics:
    auc:
      _target_: src.metrics.RocAucOne

optuna:
  epochs:
    dtype: int
    low: 10
    high: 100
  optimizer.lr:
    dtype: float
    low: !!float 1e-5
    high: !!float 5e-2
  model.reg_alphas.user:
    dtype: float
    low: !!float 1e-6
    high: !!float 1e-2
  model.reg_alphas.item:
    dtype: float
    low: !!float 1e-6
    high: !!float 1e-2
  model.reg_alphas.neg:
    dtype: float
    low: !!float 1e-6
    high: !!float 1e-2

epochs: 1
# epochs: 94  # 128 embdim

datasets:
  train:
    _target_: torch.utils.data.DataLoader
    dataset:
      _target_: src.datasets.jsonl.Iter
      path: {{ dataset }}/train.jsonl
    collate_fn:
      _target_: src.datasets.jsonl.Collator
      pad:
        - seen_items
    batch_size: 16
    shuffle: false
    pin_memory: true
    num_workers: 2
  eval:
    _target_: torch.utils.data.DataLoader
    dataset:
      _target_: src.datasets.jsonl.Iter
      path: {{ dataset }}/eval.jsonl
    collate_fn:
      _target_: experiments.bpr.dataset.OnePosCollator
      num_items: *num_items
    batch_size: 1
    shuffle: false
    pin_memory: true
    num_workers: 1

optuna_datasets:
  train:
    _target_: torch.utils.data.DataLoader
    dataset:
      _target_: src.datasets.jsonl.Iter
      path: {{ dataset }}/train.jsonl
    collate_fn:
      _target_: src.datasets.jsonl.Collator
      pad:
        - seen_items
    batch_size: 16
    shuffle: false
    pin_memory: true
    num_workers: 2
  eval:
    _target_: torch.utils.data.DataLoader
    dataset:
      _target_: src.datasets.jsonl.Iter
      path: {{ dataset }}/eval.jsonl
    collate_fn:
      _target_: experiments.bpr.dataset.OnePosCollator
      num_items: *num_items
    batch_size: 1
    shuffle: false
    pin_memory: true
    num_workers: 1

model:
  _target_: src.models.bpr.Model
  fuse_forward: true
  logits_model:
    _target_: src.models.bpr.MF
    item_bias: {{ item_bias | default(true, true) }}
    user_bias: false
    user_emb:
      _target_: torch.nn.Embedding
      num_embeddings: *num_users
      embedding_dim: {{ embedding_dim | int }}
      padding_idx: 0
    item_emb:
      _target_: torch.nn.Embedding
      num_embeddings: *num_items
      embedding_dim: {{ embedding_dim | int }}
      padding_idx: 0
  reg_alphas:
    user: 0.0025
    # user: 0.004459686555199224  # 128 embdim
    item: 0.0025
    # item: 0.00005803908333330523  # 128 embdim
    neg: 0.00025
    # neg: 0.0059752414343157475  # 128 embdim

optimizer:
  _partial_: true
  _target_: torch.optim.SGD
  lr: 0.05
  # lr: 0.0358185390941126  # 128 embdim
