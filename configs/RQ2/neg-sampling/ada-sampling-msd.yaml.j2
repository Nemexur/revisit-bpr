---
num_users: &num_users {{ (num_users | int) + 1 }}
num_items: &num_items {{ (num_items | int) + 1 }}

epochs: {{ epochs | default(70, true) | int }}

experiment:
  _target_: experiments.bpr.Experiment
  early_stopping_metric: ndcg@100
  early_stopping_patience: 13
  adaptive_sampling_prob: !!float {{ 1 / 100 | float }}
  # adaptive_sampling_prob: !!float {{ 1 / 100 | float }}  # 1024 embdim
  metrics:
    ndcg@100:
      _target_: src.metrics.NDCG
      topk: 100
    recall@100:
      _target_: src.metrics.Recall
      topk: 100
    ndcg@10:
      _target_: src.metrics.NDCG
      topk: 10
    recall@10:
      _target_: src.metrics.Recall
      topk: 10
    auc:
      _target_: src.metrics.RocAucManySlow
    ndcg@5:
      _target_: src.metrics.NDCG
      topk: 5
    recall@5:
      _target_: src.metrics.Recall
      topk: 5
    recall@20:
      _target_: src.metrics.Recall
      topk: 20
    ndcg@50:
      _target_: src.metrics.NDCG
      topk: 50
    recall@50:
      _target_: src.metrics.Recall
      topk: 50
    precision@5:
      _target_: src.metrics.Precision
      topk: 5
    precision@10:
      _target_: src.metrics.Precision
      topk: 10
    precision@50:
      _target_: src.metrics.Precision
      topk: 50
    precision@100:
      _target_: src.metrics.Precision
      topk: 100

optuna:
  experiment.adaptive_sampling_prob:
    dtype: categorical
    choices:
      - !!float {{ 1 / 10 | float }}
      - !!float {{ 1 / 50 | float }}
      - !!float {{ 1 / 100 | float }}
      - !!float {{ 1 / 500 | float }}
      - !!float {{ 1 / 700 | float }}
      - !!float {{ 1 / 1000 | float }}
      - !!float {{ 1 / 1500 | float }}
      - !!float {{ 1 / 2000 | float }}
      - !!float {{ 1 / 3000 | float }}
      - !!float {{ 1 / 4000 | float }}
  optimizer.lr:
    dtype: float
    low: !!float 1e-5
    high: !!float 5e-2

datasets:
  train:
    _target_: torch.utils.data.DataLoader
    dataset:
      _target_: experiments.bpr.dataset.SparseSamplingInMemoryWithCollator
      path: {{ dataset }}/full-train-with-fold-in.jsonl
      seen_items_path: {{ dataset }}/full-train-with-fold-in-user-seen-items.jsonl
      num_users: *num_users
      num_items: *num_items
      put_on_cuda: true
    batch_size: {{ train_batch_size | int }}
    shuffle: true
  eval:
    _target_: torch.utils.data.DataLoader
    dataset:
      _target_: experiments.bpr.dataset.InMemory
      path: {{ dataset }}/test-grouped.jsonl
      seen_items_path: {{ dataset }}/full-train-with-fold-in-user-seen-items.jsonl
    collate_fn:
      _target_: experiments.bpr.dataset.AllItemsCollator
      num_items: *num_items
    batch_size: 128
    shuffle: false
    pin_memory: true
    num_workers: 3

optuna_datasets:
  train:
    _target_: torch.utils.data.DataLoader
    dataset:
      _target_: experiments.bpr.dataset.SparseSamplingInMemoryWithCollator
      path: {{ dataset }}/train-with-fold-in.jsonl
      seen_items_path: {{ dataset }}/train-with-fold-in-user-seen-items.jsonl
      num_users: *num_users
      num_items: *num_items
      put_on_cuda: true
    batch_size: {{ train_batch_size | int }}
    shuffle: true
  eval:
    _target_: torch.utils.data.DataLoader
    dataset:
      _target_: experiments.bpr.dataset.Iter
      path: {{ dataset }}/eval-grouped.jsonl
      seen_items_path: {{ dataset }}/train-with-fold-in-user-seen-items.jsonl
    collate_fn:
      _target_: experiments.bpr.dataset.AllItemsCollator
      num_items: *num_items
    batch_size: 128
    shuffle: false
    pin_memory: true
    num_workers: 3

model:
  _target_: src.models.bpr.Model
  fuse_forward: true
  logits_model:
    _target_: src.models.bpr.MF
    item_bias: {{ item_bias | default(true, true) }}
    user_bias: false
    user_emb:
      _target_: torch.nn.Embedding
      num_embeddings: *num_users
      embedding_dim: {{ embedding_dim | int }}
      padding_idx: 0
    item_emb:
      _target_: torch.nn.Embedding
      num_embeddings: *num_items
      embedding_dim: {{ embedding_dim | int }}
      padding_idx: 0
  reg_alphas:
    all: 0.00043

optimizer:
  _partial_: true
  _target_: torch.optim.SGD
  lr: 0.001
  # lr: 0.04032600308263426  # 1024 embdim
